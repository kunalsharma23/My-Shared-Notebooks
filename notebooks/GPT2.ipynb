{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8684,
     "status": "ok",
     "timestamp": 1597145630389,
     "user": {
      "displayName": "Himanshu Bhatnagar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjcLQOcvtW2VOuFf483XzFOiCFKEfIcBrqo7VDUXw=s64",
      "userId": "14281182455951242349"
     },
     "user_tz": -330
    },
    "id": "vRHQ3maoxFvB",
    "outputId": "a80cba48-118a-4621-8551-d816615c414e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'gpt-2'...\n",
      "remote: Enumerating objects: 230, done.\u001b[K\n",
      "Receiving objects:   0% (1/230)   \r",
      "Receiving objects:   1% (3/230)   \r",
      "Receiving objects:   2% (5/230)   \r",
      "Receiving objects:   3% (7/230)   \r",
      "Receiving objects:   4% (10/230)   \r",
      "Receiving objects:   5% (12/230)   \r",
      "Receiving objects:   6% (14/230)   \r",
      "Receiving objects:   7% (17/230)   \r",
      "Receiving objects:   8% (19/230)   \r",
      "Receiving objects:   9% (21/230)   \r",
      "Receiving objects:  10% (23/230)   \r",
      "Receiving objects:  11% (26/230)   \r",
      "Receiving objects:  12% (28/230)   \r",
      "Receiving objects:  13% (30/230)   \r",
      "Receiving objects:  14% (33/230)   \r",
      "Receiving objects:  15% (35/230)   \r",
      "Receiving objects:  16% (37/230)   \r",
      "Receiving objects:  17% (40/230)   \r",
      "Receiving objects:  18% (42/230)   \r",
      "Receiving objects:  19% (44/230)   \r",
      "Receiving objects:  20% (46/230)   \r",
      "Receiving objects:  21% (49/230)   \r",
      "Receiving objects:  22% (51/230)   \r",
      "Receiving objects:  23% (53/230)   \r",
      "Receiving objects:  24% (56/230)   \r",
      "Receiving objects:  25% (58/230)   \r",
      "Receiving objects:  26% (60/230)   \r",
      "Receiving objects:  27% (63/230)   \r",
      "Receiving objects:  28% (65/230)   \r",
      "Receiving objects:  29% (67/230)   \r",
      "Receiving objects:  30% (69/230)   \r",
      "Receiving objects:  31% (72/230)   \r",
      "Receiving objects:  32% (74/230)   \r",
      "Receiving objects:  33% (76/230)   \r",
      "Receiving objects:  34% (79/230)   \r",
      "Receiving objects:  35% (81/230)   \r",
      "Receiving objects:  36% (83/230)   \r",
      "Receiving objects:  37% (86/230)   \r",
      "Receiving objects:  38% (88/230)   \r",
      "Receiving objects:  39% (90/230)   \r",
      "Receiving objects:  40% (92/230)   \r",
      "Receiving objects:  41% (95/230)   \r",
      "Receiving objects:  42% (97/230)   \r",
      "remote: Total 230 (delta 0), reused 0 (delta 0), pack-reused 230\u001b[K\n",
      "Receiving objects:  43% (99/230)   \r",
      "Receiving objects:  44% (102/230)   \r",
      "Receiving objects:  45% (104/230)   \r",
      "Receiving objects:  46% (106/230)   \r",
      "Receiving objects:  47% (109/230)   \r",
      "Receiving objects:  48% (111/230)   \r",
      "Receiving objects:  49% (113/230)   \r",
      "Receiving objects:  50% (115/230)   \r",
      "Receiving objects:  51% (118/230)   \r",
      "Receiving objects:  52% (120/230)   \r",
      "Receiving objects:  53% (122/230)   \r",
      "Receiving objects:  54% (125/230)   \r",
      "Receiving objects:  55% (127/230)   \r",
      "Receiving objects:  56% (129/230)   \r",
      "Receiving objects:  57% (132/230)   \r",
      "Receiving objects:  58% (134/230)   \r",
      "Receiving objects:  59% (136/230)   \r",
      "Receiving objects:  60% (138/230)   \r",
      "Receiving objects:  61% (141/230)   \r",
      "Receiving objects:  62% (143/230)   \r",
      "Receiving objects:  63% (145/230)   \r",
      "Receiving objects:  64% (148/230)   \r",
      "Receiving objects:  65% (150/230)   \r",
      "Receiving objects:  66% (152/230)   \r",
      "Receiving objects:  67% (155/230)   \r",
      "Receiving objects:  68% (157/230)   \r",
      "Receiving objects:  69% (159/230)   \r",
      "Receiving objects:  70% (161/230)   \r",
      "Receiving objects:  71% (164/230)   \r",
      "Receiving objects:  72% (166/230)   \r",
      "Receiving objects:  73% (168/230)   \r",
      "Receiving objects:  74% (171/230)   \r",
      "Receiving objects:  75% (173/230)   \r",
      "Receiving objects:  76% (175/230)   \r",
      "Receiving objects:  77% (178/230)   \r",
      "Receiving objects:  78% (180/230)   \r",
      "Receiving objects:  79% (182/230)   \r",
      "Receiving objects:  80% (184/230)   \r",
      "Receiving objects:  81% (187/230)   \r",
      "Receiving objects:  82% (189/230)   \r",
      "Receiving objects:  83% (191/230)   \r",
      "Receiving objects:  84% (194/230)   \r",
      "Receiving objects:  85% (196/230)   \r",
      "Receiving objects:  86% (198/230)   \r",
      "Receiving objects:  87% (201/230)   \r",
      "Receiving objects:  88% (203/230)   \r",
      "Receiving objects:  89% (205/230)   \r",
      "Receiving objects:  90% (207/230)   \r",
      "Receiving objects:  91% (210/230)   \r",
      "Receiving objects:  92% (212/230)   \r",
      "Receiving objects:  93% (214/230)   \r",
      "Receiving objects:  94% (217/230)   \r",
      "Receiving objects:  95% (219/230)   \r",
      "Receiving objects:  96% (221/230)   \r",
      "Receiving objects:  97% (224/230)   \r",
      "Receiving objects:  98% (226/230)   \r",
      "Receiving objects:  99% (228/230)   \r",
      "Receiving objects: 100% (230/230)   \r",
      "Receiving objects: 100% (230/230), 4.38 MiB | 18.30 MiB/s, done.\n",
      "Resolving deltas:   0% (0/118)   \r",
      "Resolving deltas:   6% (8/118)   \r",
      "Resolving deltas:  10% (12/118)   \r",
      "Resolving deltas:  11% (14/118)   \r",
      "Resolving deltas:  12% (15/118)   \r",
      "Resolving deltas:  16% (20/118)   \r",
      "Resolving deltas:  17% (21/118)   \r",
      "Resolving deltas:  20% (24/118)   \r",
      "Resolving deltas:  23% (28/118)   \r",
      "Resolving deltas:  27% (32/118)   \r",
      "Resolving deltas:  28% (34/118)   \r",
      "Resolving deltas:  32% (38/118)   \r",
      "Resolving deltas:  33% (40/118)   \r",
      "Resolving deltas:  34% (41/118)   \r",
      "Resolving deltas:  38% (46/118)   \r",
      "Resolving deltas:  43% (51/118)   \r",
      "Resolving deltas:  44% (52/118)   \r",
      "Resolving deltas:  45% (54/118)   \r",
      "Resolving deltas:  47% (56/118)   \r",
      "Resolving deltas:  48% (57/118)   \r",
      "Resolving deltas:  50% (60/118)   \r",
      "Resolving deltas:  54% (64/118)   \r",
      "Resolving deltas:  55% (65/118)   \r",
      "Resolving deltas:  57% (68/118)   \r",
      "Resolving deltas:  65% (77/118)   \r",
      "Resolving deltas:  68% (81/118)   \r",
      "Resolving deltas:  73% (87/118)   \r",
      "Resolving deltas:  77% (92/118)   \r",
      "Resolving deltas:  82% (97/118)   \r",
      "Resolving deltas:  97% (115/118)   \r",
      "Resolving deltas: 100% (118/118)   \r",
      "Resolving deltas: 100% (118/118), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/openai/gpt-2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8421,
     "status": "ok",
     "timestamp": 1597145701147,
     "user": {
      "displayName": "Himanshu Bhatnagar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjcLQOcvtW2VOuFf483XzFOiCFKEfIcBrqo7VDUXw=s64",
      "userId": "14281182455951242349"
     },
     "user_tz": -330
    },
    "id": "Sf8YrWFYTRlv"
   },
   "outputs": [],
   "source": [
    "!cd gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1412,
     "status": "ok",
     "timestamp": 1597145741418,
     "user": {
      "displayName": "Himanshu Bhatnagar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjcLQOcvtW2VOuFf483XzFOiCFKEfIcBrqo7VDUXw=s64",
      "userId": "14281182455951242349"
     },
     "user_tz": -330
    },
    "id": "ejJEwL7JTZKM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/gpt-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 41723,
     "status": "ok",
     "timestamp": 1597146614698,
     "user": {
      "displayName": "Himanshu Bhatnagar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjcLQOcvtW2VOuFf483XzFOiCFKEfIcBrqo7VDUXw=s64",
      "userId": "14281182455951242349"
     },
     "user_tz": -330
    },
    "id": "2o3DC-O2Wea1",
    "outputId": "194770bf-00c4-4203-8167-011d71c0defe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.12.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cc/ca70b78087015d21c5f3f93694107f34ebccb3be9624385a911d4b52ecef/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (83.1MB)\n",
      "\u001b[K     |████████████████████████████████| 83.1MB 55kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.18.5)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.1.2)\n",
      "Collecting tensorboard<1.13.0,>=1.12.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1MB 30.0MB/s \n",
      "\u001b[?25hCollecting keras-applications>=1.0.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.3.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.30.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.9.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.34.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (3.12.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (1.0.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12.0) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12.0) (49.2.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (3.1.0)\n",
      "Installing collected packages: tensorboard, keras-applications, tensorflow\n",
      "  Found existing installation: tensorboard 2.3.0\n",
      "    Uninstalling tensorboard-2.3.0:\n",
      "      Successfully uninstalled tensorboard-2.3.0\n",
      "  Found existing installation: tensorflow 2.3.0\n",
      "    Uninstalling tensorflow-2.3.0:\n",
      "      Successfully uninstalled tensorflow-2.3.0\n",
      "Successfully installed keras-applications-1.0.8 tensorboard-1.12.2 tensorflow-1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18092,
     "status": "ok",
     "timestamp": 1597145761598,
     "user": {
      "displayName": "Himanshu Bhatnagar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjcLQOcvtW2VOuFf483XzFOiCFKEfIcBrqo7VDUXw=s64",
      "userId": "14281182455951242349"
     },
     "user_tz": -330
    },
    "id": "mE9MpwAszAD1",
    "outputId": "ae25cc04-b976-404c-fd0e-71faf9a7f2fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fire>=0.1.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz (81kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 2.3MB/s \n",
      "\u001b[?25hCollecting regex==2017.4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
      "\u001b[K     |████████████████████████████████| 604kB 8.7MB/s \n",
      "\u001b[?25hCollecting requests==2.21.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 8.4MB/s \n",
      "\u001b[?25hCollecting tqdm==4.31.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2020.6.20)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
      "Collecting idna<2.9,>=2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
      "Building wheels for collected packages: fire, regex\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=04cd7527d809fa8017104787911751cc6b1f8f7a25f18dca952dc44ff49da00f\n",
      "  Stored in directory: /root/.cache/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533206 sha256=863384ec98f7b1045e33345808a6a4b79b2e00afdf7193644bfde83d3762cd01\n",
      "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
      "Successfully built fire regex\n",
      "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: fire, regex, idna, requests, tqdm\n",
      "  Found existing installation: regex 2019.12.20\n",
      "    Uninstalling regex-2019.12.20:\n",
      "      Successfully uninstalled regex-2019.12.20\n",
      "  Found existing installation: idna 2.10\n",
      "    Uninstalling idna-2.10:\n",
      "      Successfully uninstalled idna-2.10\n",
      "  Found existing installation: requests 2.23.0\n",
      "    Uninstalling requests-2.23.0:\n",
      "      Successfully uninstalled requests-2.23.0\n",
      "  Found existing installation: tqdm 4.41.1\n",
      "    Uninstalling tqdm-4.41.1:\n",
      "      Successfully uninstalled tqdm-4.41.1\n",
      "Successfully installed fire-0.3.1 idna-2.8 regex-2017.4.5 requests-2.21.0 tqdm-4.31.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9999,
     "status": "ok",
     "timestamp": 1597146713610,
     "user": {
      "displayName": "Himanshu Bhatnagar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjcLQOcvtW2VOuFf483XzFOiCFKEfIcBrqo7VDUXw=s64",
      "userId": "14281182455951242349"
     },
     "user_tz": -330
    },
    "id": "qdoOjNxnXG9z",
    "outputId": "613ace39-1491-421d-f412-eeaa8cd875e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Fetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\r",
      "Fetching checkpoint: 1.00kit [00:00, 616kit/s]                                                      \n",
      "\r",
      "Fetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\r",
      "Fetching encoder.json: 1.04Mit [00:00, 53.5Mit/s]                                                   \n",
      "Fetching hparams.json: 1.00kit [00:00, 705kit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:06, 71.4Mit/s]                                  \n",
      "Fetching model.ckpt.index: 6.00kit [00:00, 5.15Mit/s]                                               \n",
      "Fetching model.ckpt.meta: 472kit [00:00, 47.7Mit/s]                                                 \n",
      "Fetching vocab.bpe: 457kit [00:00, 46.7Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "!python3 download_model.py 124M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26177,
     "status": "ok",
     "timestamp": 1597146737939,
     "user": {
      "displayName": "Himanshu Bhatnagar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjcLQOcvtW2VOuFf483XzFOiCFKEfIcBrqo7VDUXw=s64",
      "userId": "14281182455951242349"
     },
     "user_tz": -330
    },
    "id": "IXI7eNopXLHC",
    "outputId": "a47e8a61-d2a1-4a73-c909-25c903f8249e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Fetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\r",
      "Fetching checkpoint: 1.00kit [00:00, 777kit/s]                                                      \n",
      "\r",
      "Fetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\r",
      "Fetching encoder.json: 1.04Mit [00:00, 50.9Mit/s]                                                   \n",
      "Fetching hparams.json: 1.00kit [00:00, 799kit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:21, 64.9Mit/s]                                 \n",
      "Fetching model.ckpt.index: 11.0kit [00:00, 3.87Mit/s]                                               \n",
      "Fetching model.ckpt.meta: 927kit [00:00, 31.4Mit/s]                                                 \n",
      "Fetching vocab.bpe: 457kit [00:00, 45.2Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "!python3 download_model.py 355M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 70145,
     "status": "ok",
     "timestamp": 1597146787254,
     "user": {
      "displayName": "Himanshu Bhatnagar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjcLQOcvtW2VOuFf483XzFOiCFKEfIcBrqo7VDUXw=s64",
      "userId": "14281182455951242349"
     },
     "user_tz": -330
    },
    "id": "uC_ESsXHXMvq",
    "outputId": "6dfb4bf3-e3b3-481c-8b99-f5f967043bbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Fetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\r",
      "Fetching checkpoint: 1.00kit [00:00, 664kit/s]                                                      \n",
      "\r",
      "Fetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\r",
      "Fetching encoder.json: 1.04Mit [00:00, 57.3Mit/s]                                                   \n",
      "Fetching hparams.json: 1.00kit [00:00, 918kit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 3.10Git [00:47, 65.5Mit/s]                                 \n",
      "Fetching model.ckpt.index: 16.0kit [00:00, 9.04Mit/s]                                               \n",
      "Fetching model.ckpt.meta: 1.38Mit [00:00, 63.2Mit/s]                                                \n",
      "Fetching vocab.bpe: 457kit [00:00, 40.5Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "!python3 download_model.py 774M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 98840,
     "status": "ok",
     "timestamp": 1597146886107,
     "user": {
      "displayName": "Himanshu Bhatnagar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjcLQOcvtW2VOuFf483XzFOiCFKEfIcBrqo7VDUXw=s64",
      "userId": "14281182455951242349"
     },
     "user_tz": -330
    },
    "id": "kwmc45XsXOEW",
    "outputId": "2cea9e6c-981d-48d0-982c-36aeb11868c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Fetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\r",
      "Fetching checkpoint: 1.00kit [00:00, 724kit/s]                                                      \n",
      "\r",
      "Fetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\r",
      "Fetching encoder.json: 1.04Mit [00:00, 56.5Mit/s]                                                   \n",
      "Fetching hparams.json: 1.00kit [00:00, 753kit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 6.23Git [01:34, 65.9Mit/s]                                 \n",
      "Fetching model.ckpt.index: 21.0kit [00:00, 12.0Mit/s]                                               \n",
      "Fetching model.ckpt.meta: 1.84Mit [00:00, 55.5Mit/s]                                                \n",
      "Fetching vocab.bpe: 457kit [00:00, 51.2Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "!python3 download_model.py 1558M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6308,
     "status": "ok",
     "timestamp": 1597147398075,
     "user": {
      "displayName": "Himanshu Bhatnagar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjcLQOcvtW2VOuFf483XzFOiCFKEfIcBrqo7VDUXw=s64",
      "userId": "14281182455951242349"
     },
     "user_tz": -330
    },
    "id": "l3b88hMGZt2C"
   },
   "outputs": [],
   "source": [
    "#model.py\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.training import HParams\n",
    "\n",
    "def default_hparams():\n",
    "    return HParams(\n",
    "        n_vocab=0,\n",
    "        n_ctx=1024,\n",
    "        n_embd=768,\n",
    "        n_head=12,\n",
    "        n_layer=12,\n",
    "    )\n",
    "\n",
    "def shape_list(x):\n",
    "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(x)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x = x - tf.reduce_max(x, axis=axis, keepdims=True)\n",
    "    ex = tf.exp(x)\n",
    "    return ex / tf.reduce_sum(ex, axis=axis, keepdims=True)\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "\n",
    "def norm(x, scope, *, axis=-1, epsilon=1e-5):\n",
    "    \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        n_state = x.shape[-1].value\n",
    "        g = tf.get_variable('g', [n_state], initializer=tf.constant_initializer(1))\n",
    "        b = tf.get_variable('b', [n_state], initializer=tf.constant_initializer(0))\n",
    "        u = tf.reduce_mean(x, axis=axis, keepdims=True)\n",
    "        s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)\n",
    "        x = (x - u) * tf.rsqrt(s + epsilon)\n",
    "        x = x*g + b\n",
    "        return x\n",
    "\n",
    "def split_states(x, n):\n",
    "    \"\"\"Reshape the last dimension of x into [n, x.shape[-1]/n].\"\"\"\n",
    "    *start, m = shape_list(x)\n",
    "    return tf.reshape(x, start + [n, m//n])\n",
    "\n",
    "def merge_states(x):\n",
    "    \"\"\"Smash the last two dimensions of x into a single dimension.\"\"\"\n",
    "    *start, a, b = shape_list(x)\n",
    "    return tf.reshape(x, start + [a*b])\n",
    "\n",
    "def conv1d(x, scope, nf, *, w_init_stdev=0.02):\n",
    "    with tf.variable_scope(scope):\n",
    "        *start, nx = shape_list(x)\n",
    "        w = tf.get_variable('w', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))\n",
    "        b = tf.get_variable('b', [nf], initializer=tf.constant_initializer(0))\n",
    "        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
    "        return c\n",
    "\n",
    "def attention_mask(nd, ns, *, dtype):\n",
    "    \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
    "    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
    "    \"\"\"\n",
    "    i = tf.range(nd)[:,None]\n",
    "    j = tf.range(ns)\n",
    "    m = i >= j - ns + nd\n",
    "    return tf.cast(m, dtype)\n",
    "\n",
    "\n",
    "def attn(x, scope, n_state, *, past, hparams):\n",
    "    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n",
    "    assert n_state % hparams.n_head == 0\n",
    "    if past is not None:\n",
    "        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n",
    "\n",
    "    def split_heads(x):\n",
    "        # From [batch, sequence, features] to [batch, heads, sequence, features]\n",
    "        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])\n",
    "\n",
    "    def merge_heads(x):\n",
    "        # Reverse of split_heads\n",
    "        return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n",
    "\n",
    "    def mask_attn_weights(w):\n",
    "        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
    "        _, _, nd, ns = shape_list(w)\n",
    "        b = attention_mask(nd, ns, dtype=w.dtype)\n",
    "        b = tf.reshape(b, [1, 1, nd, ns])\n",
    "        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n",
    "        return w\n",
    "\n",
    "    def multihead_attn(q, k, v):\n",
    "        # q, k, v have shape [batch, heads, sequence, features]\n",
    "        w = tf.matmul(q, k, transpose_b=True)\n",
    "        w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))\n",
    "\n",
    "        w = mask_attn_weights(w)\n",
    "        w = softmax(w)\n",
    "        a = tf.matmul(w, v)\n",
    "        return a\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        c = conv1d(x, 'c_attn', n_state*3)\n",
    "        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n",
    "        present = tf.stack([k, v], axis=1)\n",
    "        if past is not None:\n",
    "            pk, pv = tf.unstack(past, axis=1)\n",
    "            k = tf.concat([pk, k], axis=-2)\n",
    "            v = tf.concat([pv, v], axis=-2)\n",
    "        a = multihead_attn(q, k, v)\n",
    "        a = merge_heads(a)\n",
    "        a = conv1d(a, 'c_proj', n_state)\n",
    "        return a, present\n",
    "\n",
    "\n",
    "def mlp(x, scope, n_state, *, hparams):\n",
    "    with tf.variable_scope(scope):\n",
    "        nx = x.shape[-1].value\n",
    "        h = gelu(conv1d(x, 'c_fc', n_state))\n",
    "        h2 = conv1d(h, 'c_proj', nx)\n",
    "        return h2\n",
    "\n",
    "\n",
    "def block(x, scope, *, past, hparams):\n",
    "    with tf.variable_scope(scope):\n",
    "        nx = x.shape[-1].value\n",
    "        a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n",
    "        x = x + a\n",
    "        m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)\n",
    "        x = x + m\n",
    "        return x, present\n",
    "\n",
    "def past_shape(*, hparams, batch_size=None, sequence=None):\n",
    "    return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]\n",
    "\n",
    "def expand_tile(value, size):\n",
    "    \"\"\"Add a new axis of given size.\"\"\"\n",
    "    value = tf.convert_to_tensor(value, name='value')\n",
    "    ndims = value.shape.ndims\n",
    "    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n",
    "\n",
    "def positions_for(tokens, past_length):\n",
    "    batch_size = tf.shape(tokens)[0]\n",
    "    nsteps = tf.shape(tokens)[1]\n",
    "    return expand_tile(past_length + tf.range(nsteps), batch_size)\n",
    "\n",
    "\n",
    "def model(hparams, X, past=None, scope='model', reuse=False):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        results = {}\n",
    "        batch, sequence = shape_list(X)\n",
    "\n",
    "        wpe = tf.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
    "                             initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
    "                             initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        past_length = 0 if past is None else tf.shape(past)[-2]\n",
    "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
    "\n",
    "        # Transformer\n",
    "        presents = []\n",
    "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
    "        assert len(pasts) == hparams.n_layer\n",
    "        for layer, past in enumerate(pasts):\n",
    "            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
    "            presents.append(present)\n",
    "        results['present'] = tf.stack(presents, axis=1)\n",
    "        h = norm(h, 'ln_f')\n",
    "\n",
    "        # Language model loss.  Do tokens <n predict token n?\n",
    "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
    "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
    "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
    "        results['logits'] = logits\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1331,
     "status": "ok",
     "timestamp": 1597147529258,
     "user": {
      "displayName": "Himanshu Bhatnagar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjcLQOcvtW2VOuFf483XzFOiCFKEfIcBrqo7VDUXw=s64",
      "userId": "14281182455951242349"
     },
     "user_tz": -330
    },
    "id": "m_taMpQFaS3Y"
   },
   "outputs": [],
   "source": [
    "#encoder.py\n",
    "\"\"\"Byte pair encoding utilities\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import regex as re\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "        return text\n",
    "\n",
    "def get_encoder(model_name, models_dir):\n",
    "    with open(os.path.join(models_dir, model_name, 'encoder.json'), 'r') as f:\n",
    "        encoder = json.load(f)\n",
    "    with open(os.path.join(models_dir, model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "    return Encoder(\n",
    "        encoder=encoder,\n",
    "        bpe_merges=bpe_merges,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1244,
     "status": "ok",
     "timestamp": 1597148627142,
     "user": {
      "displayName": "Himanshu Bhatnagar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjcLQOcvtW2VOuFf483XzFOiCFKEfIcBrqo7VDUXw=s64",
      "userId": "14281182455951242349"
     },
     "user_tz": -330
    },
    "id": "4WpMW1gyayST"
   },
   "outputs": [],
   "source": [
    "#sample.py\n",
    "import tensorflow as tf\n",
    "\n",
    "#import model\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        # no truncation\n",
    "        return logits\n",
    "\n",
    "    def _top_k():\n",
    "        values, _ = tf.nn.top_k(logits, k=k)\n",
    "        min_values = values[:, -1, tf.newaxis]\n",
    "        return tf.where(\n",
    "            logits < min_values,\n",
    "            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "            logits,\n",
    "        )\n",
    "    return tf.cond(\n",
    "       tf.equal(k, 0),\n",
    "       lambda: logits,\n",
    "       lambda: _top_k(),\n",
    "    )\n",
    "\n",
    "# below function picked from \n",
    "# https://www.tutorialexample.com/sort-a-tensor-from-largest-to-smallest-in-tensorflow-tensorflow-example/\n",
    "# function was needed since gpt-2 needs tf 1.12 to run, however in tf 1.12\n",
    "# tf does not have a function sort(), so we need to fix that\n",
    "# above issue has been reported on stackoverflow as well\n",
    "# https://stackoverflow.com/questions/61510865/tensorflow-has-no-attribute-sort-in-gpt-2-git-release\n",
    "def tf_sort(tensor):\n",
    "    shape = tf.shape(tensor)\n",
    "    rank = tf.rank(tensor)\n",
    "    k_n = shape[rank-1]\n",
    "    t_v, t_i = tf.nn.top_k(tensor,k=k_n,sorted=True,name=None)\n",
    "    return t_v\n",
    "\n",
    "def top_p_logits(logits, p):\n",
    "    \"\"\"Nucleus sampling\"\"\"\n",
    "    batch, _ = logits.shape.as_list()\n",
    "    #sorted_logits = tf.sort(logits, direction='DESCENDING', axis=-1)\n",
    "    sorted_logits = tf_sort(logits)\n",
    "    cumulative_probs = tf.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
    "    indices = tf.stack([\n",
    "        tf.range(0, batch),\n",
    "        # number of indices to include\n",
    "        tf.maximum(tf.reduce_sum(tf.cast(cumulative_probs <= p, tf.int32), axis=-1) - 1, 0),\n",
    "    ], axis=-1)\n",
    "    min_values = tf.gather_nd(sorted_logits, indices)\n",
    "    return tf.where(\n",
    "        logits < min_values,\n",
    "        tf.ones_like(logits) * -1e10,\n",
    "        logits,\n",
    "    )\n",
    "\n",
    "\n",
    "def sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0, top_p=1):\n",
    "    if start_token is None:\n",
    "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "    else:\n",
    "        assert context is None, 'Specify exactly one of start_token and context!'\n",
    "        context = tf.fill([batch_size, 1], start_token)\n",
    "\n",
    "    def step(hparams, tokens, past=None):\n",
    "        #lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)\n",
    "        lm_output = model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)\n",
    "\n",
    "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
    "        presents = lm_output['present']\n",
    "        #presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))\n",
    "        presents.set_shape(past_shape(hparams=hparams, batch_size=batch_size))\n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'presents': presents,\n",
    "        }\n",
    "\n",
    "    with tf.name_scope('sample_sequence'):\n",
    "        def body(past, prev, output):\n",
    "            next_outputs = step(hparams, prev, past=past)\n",
    "            logits = next_outputs['logits'][:, -1, :]  / tf.to_float(temperature)\n",
    "            logits = top_k_logits(logits, k=top_k)\n",
    "            logits = top_p_logits(logits, p=top_p)\n",
    "            samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)\n",
    "            return [\n",
    "                next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
    "                samples,\n",
    "                tf.concat([output, samples], axis=1)\n",
    "            ]\n",
    "\n",
    "        past, prev, output = body(None, context, context)\n",
    "\n",
    "        def cond(*args):\n",
    "            return True\n",
    "\n",
    "        _, _, tokens = tf.while_loop(\n",
    "            cond=cond, body=body,\n",
    "            maximum_iterations=length - 1,\n",
    "            loop_vars=[\n",
    "                past,\n",
    "                prev,\n",
    "                output\n",
    "            ],\n",
    "            shape_invariants=[\n",
    "                #tf.TensorShape(model.past_shape(hparams=hparams, batch_size=batch_size)),\n",
    "                tf.TensorShape(past_shape(hparams=hparams, batch_size=batch_size)),\n",
    "                tf.TensorShape([batch_size, None]),\n",
    "                tf.TensorShape([batch_size, None]),\n",
    "            ],\n",
    "            back_prop=False,\n",
    "        )\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4049,
     "status": "ok",
     "timestamp": 1597160797707,
     "user": {
      "displayName": "Himanshu Bhatnagar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjcLQOcvtW2VOuFf483XzFOiCFKEfIcBrqo7VDUXw=s64",
      "userId": "14281182455951242349"
     },
     "user_tz": -330
    },
    "id": "OkWpbcgob9N1"
   },
   "outputs": [],
   "source": [
    "#generate_unconditional_samples.py\n",
    "import fire\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#import model, sample, encoder\n",
    "\n",
    "def sample_model(\n",
    "    model_name='124M',\n",
    "    seed=None,\n",
    "    nsamples=0,\n",
    "    batch_size=1,\n",
    "    length=None,\n",
    "    temperature=1,\n",
    "    top_k=0,\n",
    "    top_p=1,\n",
    "    models_dir='models',\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the sample_model\n",
    "    :model_name=124M : String, which model to use\n",
    "    :seed=None : Integer seed for random number generators, fix seed to\n",
    "     reproduce results\n",
    "    :nsamples=0 : Number of samples to return, if 0, continues to\n",
    "     generate samples indefinately.\n",
    "    :batch_size=1 : Number of batches (only affects speed/memory).\n",
    "    :length=None : Number of tokens in generated text, if None (default), is\n",
    "     determined by model hyperparameters\n",
    "    :temperature=1 : Float value controlling randomness in boltzmann\n",
    "     distribution. Lower temperature results in less random completions. As the\n",
    "     temperature approaches zero, the model will become deterministic and\n",
    "     repetitive. Higher temperature results in more random completions.\n",
    "    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
    "     considered for each step (token), resulting in deterministic completions,\n",
    "     while 40 means 40 words are considered at each step. 0 (default) is a\n",
    "     special setting meaning no restrictions. 40 generally is a good value.\n",
    "     :models_dir : path to parent folder containing model subfolders\n",
    "     (i.e. contains the <model_name> folder)\n",
    "    \"\"\"\n",
    "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "    #enc = encoder.get_encoder(model_name, models_dir)\n",
    "    enc = get_encoder(model_name, models_dir)\n",
    "    #hparams = model.default_hparams()\n",
    "    hparams = default_hparams()\n",
    "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
    "        hparams.override_from_dict(json.load(f))\n",
    "\n",
    "    if length is None:\n",
    "        length = hparams.n_ctx\n",
    "    elif length > hparams.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        #output = sample.sample_sequence(\n",
    "        output = sample_sequence(\n",
    "            hparams=hparams, length=length,\n",
    "            start_token=enc.encoder['<|endoftext|>'],\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k, top_p=top_p\n",
    "        )[:, 1:]\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "        saver.restore(sess, ckpt)\n",
    "\n",
    "        generated = 0\n",
    "\n",
    "        while nsamples == 0 or generated < nsamples:\n",
    "            out = sess.run(output)\n",
    "            for i in range(batch_size):\n",
    "                generated += batch_size\n",
    "                text = enc.decode(out[i])\n",
    "                print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "                file.write('\\n'+\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40+'\\n')\n",
    "                print(text)\n",
    "                file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 557367,
     "status": "ok",
     "timestamp": 1597163347622,
     "user": {
      "displayName": "Himanshu Bhatnagar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjcLQOcvtW2VOuFf483XzFOiCFKEfIcBrqo7VDUXw=s64",
      "userId": "14281182455951242349"
     },
     "user_tz": -330
    },
    "id": "UCQrCwGacNXI",
    "outputId": "561e822a-b3e0-474e-dad1-719324fd41ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
      "======================================== SAMPLE 1 ========================================\n",
      "Actually, Branch 10 and 11 were sort of built on nice, consistent cassettes. I still have those 12 sparkling bass bars anyway, but the seventh one is better than the second facing them. I look at the old and I can see the subtle distinction between the the new \"S\" & the Y-number at the touch of a couple of drops. From the old post, if I had to pick the prettiest post to keep-off their branding, it would be that of the old manufacturer my father had. (I believe he co-owned the express) (how credit is due to the famous ad ATLOSS and owner of SPIKE ARC? because did most of these day strong branches refer to the last company last (not ADA??) before NC Kids? or Ozmer???). How am I to see and catch a signal that being in one commits me to a causal relationship at the vendor and subsequently trade in one of their products?) and all of that things in between which no-one else I know shares that shit since that tree complex out of your trunk looks an awful lot like ORGANICHBELLY.\n",
      "\n",
      "\n",
      "Tom has brakemen marks on six of the bass bars that appear on the \"photos of stationery to which i ...\n",
      "\n",
      "\n",
      "I wn...lwxxI i...lrdllfsxxxaw realismwl I...lmsk i alssrwns 5-57t5t1l22 ii ...: • • •\n",
      "\n",
      "\n",
      "Ingoje Jetnum, free air freshener under shoulder-line norie an arioaponic degenrettente firedeonen squamos décis des... (and tuci...)\n",
      "\n",
      "\n",
      "Amy, crew49 office. locator by home on vi wharf domus....\n",
      "\n",
      "TheArtisted coat buffalo (Croatian throws!!)\n",
      "\n",
      "\n",
      "Back Koka, japp ttzolnogue promonica 24myl years ----- to question\n",
      "\n",
      "I cannot ship ten models this week. I will need to get a product finalized next week.....\n",
      "\n",
      "Tzutlor yes, since there are inserts to prime plans.... Hey, patent kokaday may be broken, but it is the exact same part as the buttons that you will see on this pedal. I broke it to unlock it!!! Also no this didn't have any internal circuit breaker sitesq matrix on it...\n",
      "\n",
      "\n",
      "all you have to do is order an old cassette reauthorized by poster.\n",
      "\n",
      "\n",
      "--SoeAe\n",
      "\n",
      "\n",
      "dza21 TYBTIN BARGER ANALUMIAS DRUNT DRAFTUNDER/SERENEM OF TOP SERF STABILIZ Athletic slide of bass belt again into wall tight container with a louvered leather, 1 push rub tube lower into other grunge steering and groove. Heavy band holding 2 plastic strap sections drenched clay film mesh spider with metal shovel held 3-4 open memory bulbs, pepper! Light panel mounted on the total grounding digital circuits amp at the outlet of the amp (cook gel!)\n",
      "\n",
      "\n",
      "CAMS being knocked off and on monitor projects 747 master 73 steering reel pieces, wood floor supports for three second level models UNIVERSAL STYLE THEME with single 8 inch check-ins sitting in double pack, 4 mo (enough room) and plugs for jack (+ parts cover and keep amp in case long group gets drains or gets stopped repeatedly!)\n",
      "\n",
      "\n",
      "Optional key roll-in-essentials and drum stand plugs (extra current cat; D2 (the gaux'mo side if assumed to be hardware) 200 x 48.5 iq. rack which gives ample room to mount quality material D. v. worldly Guate Technic D. 253 I Paula's Radio Condom Radio Company 1100 66UIFe bridges provide maximum output outside of human range which is one notch higher than current. Howard Home Audio/MI-3/CE-1 (IP-6) headphone jacks are placed in Automated picture cable system as part of Sales America manual Statewide line of Safety Reliability Model number 47105 Habuelet as hint reel wired diagnostic system and gray use program sheere RICS J.C. Fiocci space heater40 UAN Hexalt510 Assembly Engine Phaiser GT - Deluxe 5.25 inch mono drum...plug model cavalry t-board and gray commercial and marine paintia shock car stereo carrying Elontronic remote volume wolf controlled actors played Kobble speed persons (mostly QTR student. I would need ballads on freeze cut music lines to go for postured) ONE car stereo on a cage mounted on platform sample assays with SPL 3,000 +/- 94 nal band Datavis record player marketing volunteer who only talked to me because he came in television once when she was a 4B Student Carmen great time listening Italian tour...arrelet of Wangram, Burgos appearance staff great time listening book on the set People sucious to tapes written including my dissertation about the horrible high sweep vinyls Case law recorders foot stoppers and irres\n",
      "======================================== SAMPLE 2 ========================================\n",
      "Or tale short. Last July, Walker flew to Washington to honor Utah Senator Mike Lee, the state's late senator, but he missed Franken's Republican primary vote of 6.98 million likely voters.\n",
      "\n",
      "\n",
      "Even after months of a mountain of current ferment, other Democrats confirm now that reality is pretty much the only explanation.2\n",
      "\n",
      "\n",
      "\"The liberal media make a huge deal of it,\" said Doug Collins, a member of the secret calling force on civics. \"This leads to a lot of misperceptions of democracy. Whenever they tell people that from November they'd like to state their views, they get amusement from voters when they realize that they are not always politically correct, and when the media focus on voters bias to their political goals as opposed to to how they will vote on Election Day, the voter community is very badly ill-informed and is having a very serious problem with trust.\"\n",
      "\n",
      "\n",
      "Another former Democratic president Jr. recalled an incident recently where Mueller's team was trying to skew the U.S. election results to the Democrats hoping that voters would keep Haley against Chris Christie in New Jersey. Flynn believed that while in that image Flynn had \"got out of town,\" which raised even more doubts about the Democratic strategy. (Flynn also added that he \"had doubts\" about if Trump and his advisers would disagree about who to trust \"if not who.\"?) This Bush moment clearly stirred those concerns at the highest levels of the Democratic coalition — how could any Democratic nominee actually trust Democrats so dangerous to their own fortunes?3\n",
      "\n",
      "\n",
      "Even members of the trial judiciary involved in the investigation of Trump's past actions in the War on Puerto Rico said they would want to shun any comments from special counsel Robert Mueller cut to President Trump here at the Washington Post, though there were still lots of conflicting reports out there about the kind of thing Trump had said' – to target a reporter. The U.S. government's complaint and special counsel Mueller usurped Holder's instrumentality to brag about the special counsel's focus on political issues\" never included the heightened scrutiny that the complaints were getting.4\n",
      "\n",
      "\n",
      "The same dynamic would light up those who had spent decades hearing police reports and memoranda about Trump's presidency, for whom Clinton acted more criminally than more censored administrations in sealed newsrooms. Maybe it is not the Democrats' office that is driven to sparking the moment. Maybe there is something by the silence that's put out by his selection so determined to stifle crucial, politically charged discussion about our country's planned king. Fukuoka professor Matt Hyten pointed out that within national security circles, neither Democrat nor Republican seem to believe in shared truths. \"What everyone — if there is one – determines to be the politics of that election is that they want to avoid any general public discussion about who the president should be at the expense of myself, libs and important concerns about foreign affairs,\" Hyten said.\n",
      "\n",
      "\n",
      "You wonder, also, whether maybe what Trump has done over the past year is equivalent to what Sessions recused himself from when pressed about foreign relations during his confirmation hearing. But that is a kind of big \"hole\" for a president that might just 'tone down' his candidacy. But we have that 2002 Duke basketball case to blame.\n",
      "\n",
      "\n",
      "And Graves is optimistic that the real problem in this post-romantic juggernaut is that after a year of seeming fairly glum, the real things are coming through. White House counsel Robert Mueller is just about to dismiss Scott Pruitt's health-care proposal, writes Karl Rove. Under the past administration, only a few allies in the Washington establishment had Clinton up for a big push for conservative practices like free trade, annual care for the disabled, physician clinics, reproductive issues like birth control, gay marriage and making claims about Hillary Clinton's trove of money for lobbying. Newt Gingrich was welter by Grover Norquist's 2008 tax deferrals in the Republican primaries, good case year in favor of coal. Mitt Romney and Ron Paul had the support of one believer in the challenge of national security, and that of another – or research believing themselves to be Muslims enterprising intrepid wingnut.\n",
      "\n",
      "\n",
      "Hare is seeking to chill heavy dose of impulse that's so paralyzing it keeps reaching us back in waves. Both, we hope, will have a point, but there might be other reasons to push for different batshit dismissals. One, these were days when we trusted fossil fuels, corporate welfare, Big Oil, and the massive profit margin of Wall Street companies threatening national defense from drought to up to 20 percent for starters were all the representatives of the the critical citizens they represented, Pell Farrington argued in Crossfire. By endorsing Cruz to win their home state based on \"deepClimateTruth,\" firms were, according to Farrington, threatened \"a national security military super-regime that would send a fleet of AK-47 tanks, tank-insane tanks and infantry-infield vehicles and aircraft to the doorstep of our people on American soil over private property means that would liberate us of that\n",
      "======================================== SAMPLE 3 ========================================\n",
      "Certain files may overload on your system. This can happen if your computer is becoming incapable of loading content on the computer's memory. To make your system unable to load the file, you may type the file-name name in the search box under Files and folders. These special search filters must be selected when using this file manager. If a file contains invalid or corrupt content, please manually contact the URL Mediavending.org to correct it.<|endoftext|>Machines Radio Network® 9000 , the WM5 Distributed Radio System design with meaning and evolution, is fielded to recording, analyzing, then eliminating sampling synchronizations and receive external partners sublimating application traffic through the fade-out interface. The NAIL region of the architecture utilises a unique waveform analysis II based on Transport Operations Technology (TOT) digital byte polling technologies. Optimal scalability, low throughput, support for high frequency transmission by a high speed microwave piezoelectric microwave backbone (multiple-system sub-band 2.4GHz), and low cost with now superior common band 10NB transceivers logic outputs allows for position detection and pinpoint precision beyond current form factors support for advanced multisatellite solutions KEYWORDS INCLUDED M111E L Eddy-Dome System 9 L Keycru Warp Drive Inspection 5 x's Pushch-K 3-Way MN-1/L Eddy-Dome 9 0 x/False P Sync system micro-A-10 3 YTD Devices\n",
      "\n",
      "A workhorse of three ACM wavesbonding participants for spatial imbalanced operations, the WM50 Gordon is the world leader in the design and formulation of digital pulse spectrum interfaces (VMI) coupled with multi-state bipolar 12V and power density pixel headband. With offering markets with proven interconnect rate of 10.5Vlts/s, this European grouping is now well-established in architecture and full-system use. Furthermore, Professor Worriman invests his expertise in modern mass-produced dual slice diodes with working universal solutions together EXOPS spray can, solve scanning problems that spikes up low-resolution multiple-states quantum pattern particles, Spencer is Consultant with eTechnology Central across development, TV integration and technology-critical namespace-insight initiatives. The Singularity Magnetic Activity (SSMA) Challenge is branded the hottest high-tech need point in nanotechnology and gets all high-profile applications associated with the group. Constantly upgrading the building block(s) of disruptive systems is a priority for the Multithreaded ACM can design. The WM50 central datasheet demonstrates the Solutions Elevator, RS 8.1 NE LabmaMeAT 1.AT 0GC/2UME 9.230 GTB.<|endoftext|>Nobe Aboom is not convinced by Cruz's claims about factory injuries after suffering a neck injury while assigned his command duties on Tuesday.\n",
      "\n",
      "The announcer visiting longstanding Cruz hotel off Monroe said he upheld his dignity by insisting \"you shouldn't have neck injuries\" and by saying that \"you seriously didn't want to be there.\"\n",
      "\n",
      "The campaign manager, Frank Vogel, said Aboom was fellated and fired from the campaign Saturday.\n",
      "\n",
      "\"The RNC Joe FitzGerald administered an emergency call. Domestic trauma-informed party hires that didn't do what Joe Geragos, Cruz Assistants, warranted,\" he said in a memo released Tuesday afternoon.\n",
      "\n",
      "Aboom's success in his campaign-allied quest to win outside observers to vote against Ted Cruz's name, allies in the hotel and elsewhere are plotting an end to the campaign's plug and play strategy.\n",
      "\n",
      "Moore apparently has always been slack on the Cruz camp as part of the forum of Biden interview questions laid out behind closed doors. Chamber signs outside the event Sunday say: \"Call at one of Hell's Bush Showrooms.\"\n",
      "\n",
      "There's little that abounded about Aboom throughout the day, Bob Beckel told reporters at the RNC that Walker needs to spend more time with Embraham's family, and refusing to call Cruz and wife, mom and much, much more.\n",
      "\n",
      "Rather than focus on the negligible attention Cruz is getting, Beasley wants Cruz to get in \"strong\" and will continue making attacks about reality Sunday.\n",
      "\n",
      "Con in Congress, not rig for Cruz, they're joint watchers, Beasley said. They'll rage about what Obamacare allows, or ask Cruz for a fix and McConnell to make the Dems do it for them.\n",
      "\n",
      "In the summer of 2015, Cruz gave a talk on polling from a crowd of conservative donors at a Topeka conference, worked to allow television ratings for his 2016 festivities, attracted the Karl Rove crowd DMP Kylero Richter wife Michelle Team Walker bill, raised about $244k total as He feuded with Sen. Bob Corker on Wednesday night over his ties to Russia and chairman of the Rand Paul-John Paul Senate Leadership Fund.\n",
      "\n",
      "Then he claimed, after some foreign hawk-ish commentary from that crowd, that \"Tuesday's primaries are technology for opportunity\" and was even\n",
      "======================================== SAMPLE 4 ========================================\n",
      "A comics fan is getting pocket fobs to play piano, before the subject of microchips is ever mentioned. DX Printers offer the ability to check on yourself while you're glancing through their intercoms only by glancing your fingers at a recording of an episode you already checked. After the end credits, a number of computers give you a non-stop hit \"M\" signal that turns your sina paper into segments of the screen broadcast to your local news station.\n",
      "\n",
      "Others, hoping to grow from the primitive to the more enthusiastic, are giving you virtual stereo, even though sound amplifiers are commonly used in field recordings. Working now uses the microchips in fine art glasses. Who isn't tired of hearing songs and checking music wondering what it's really like to educate yourself on everything from those from Roseanne to \"The Simpsons.\" (If mono are your thing, you can always snap shut your multi-channel head-band and find something to play while soaking in the grand artistic bliss.)\n",
      "\n",
      "Mr.Sirs from the Netflix video provider dubbed \"TechnGravis,\" who runs the site, has opened up his Bing search to many of the existing feeds as machines might work better sans microphones (also known as allotment \"back-end cams\"), making looking at thousands of clips the most style defining event and a wide-ranging collection of podcasts for writing without having to steal would-be anthrax sufferers from elsewhere on the site.\n",
      "\n",
      "Premiere\n",
      "\n",
      "In other words, treating your little moments like a baseball game with 44 notes and counting might just satisfy a pelf mood — like watching other people resume printing after the game was over. HDMI didn't work much during W.C. # 1-watch crossover thanks to a glitch in software that stopped the sound distorting the recording from the show downsampling, but all tape sounds were made to even out the jiggy audio. That might be enough to satisfy you.\n",
      "\n",
      "The BBC is now abandoning its service for a third generation application traded called \"Podcast Reports.\" Now Hulu and Amazon watch streaming videos and video files on your computer remotely from or through a tachometer. TruVOX available is slightly nicer than your standard AV receiver's on VST or lossless, high-yield stereo where low statements can be forked off and lost into a resampled tape at a fraction of the cost of broadcasting though the local system using Google Earth so no episodic experience is necessary.\n",
      "\n",
      "Things aren't easy to get a grip on as the technology slowly loses its cool, but even having a remote point to use can close the door a la HBO Go.\n",
      "\n",
      "Netflix\n",
      "\n",
      "A buff from this College Dropout favorite is Netflix. The software for your computer takes you to actions, from backup or downloading, as you use the site to stream your hatred of Herman Bader's 1972 short series, Hard Counting on El Capitan, to filming ONLY in two languages. If you speak the English language, most of your material begins with responses about war, warfare, political commentary, sex and videos this year.\n",
      "\n",
      "You can also turn your computer to report and Put watched together shows from the past year directly to the Netflix offer. For $20 (free in the United States and $21 internationally), you can stream between 12 and 16 episodes of werewolf with 10 seconds to spare, a jukebox, your original transcript and derivation tweets from correct sources included from a wide range of sources. Additionally there's a video feed available, so you don't have to wait for your security team just yet thereafter to dig deeper into your current work.\n",
      "\n",
      "Explanation MLS-style sidebars now function at some of the easiest points on the site for a Google search even when you're browsing an 8:1 resolution and more, using Click to drill down what keywords appear in my chronological home directories now and then. These feeds suffocate the posts described in the previous paragraphs, including a picture of his elaborated... not so much on both podcasts as sectors into his work, exploring every note he spent his days on.\n",
      "\n",
      "The general rule tune at click (with cautious cursor movement) lordouted out any damning hasty data about headlines that bejozer (or thought about word, obviously) dominated more times of the year than were pertinent compared to last week's article. My author War graduates from University College London at \"very sad thinking of the new rights the FBI lack that previously receive them,\" blogging about his now-passlaw education so as not to eclipse NYU's. Mika Kawata is comfortably in third place, despite the less-than-indulgent vibe of his new prediction, allowing Pooh to narrate the 2001 San Diego Chargers. Butt (NBC feed!) adverts didn't grab you a long plane ride but were Kal Penn and Benny Benowicz that much mean? Plus, audio-assigned quality ratings weren't, first and second raps are always and perfectly suitable for larger venues. San Diego\n",
      "======================================== SAMPLE 5 ========================================\n",
      "PUBLICATIONS ORDERING INFORMATION:\n",
      "\n",
      "To purchase available publications prepared by the Office of the Revisor, please click the \"Publications\" link at the top of the page and review the \"Payment Methods and Fees\" for ordering instructions.\n",
      "\n",
      "INFORMATION ABOUT THIS WEBSITE:\n",
      "\n",
      "The Revised Statutes of Missouri are now electronically available on this site in an updated format that more accurately reflects the statutory format in our print publication.\n",
      "\n",
      "Certain features of the former website (MOGA - see link at bottom of page) remain available on that website for your convenience.\n",
      "\n",
      "The statutes posted on this website are uncertified to the exclusion of any implication of flagrant copyright infringement.\n",
      "\n",
      "A link to the original published trial law may be obtained at the link on the original site link or by emailing education@codereview.org.<|endoftext|>Say thanks by giving cjs_illospend a tip and help them continue to share amazing Things with the Thingiverse community.\n",
      "\n",
      "We're sure cjs_illospend would love to see what you've printed. Please document your print and share a Make with the community.\n",
      "\n",
      "To post a Make simply visit this Thing again and click Post a Make to start uploading your photo. It's even easier to post a Make via the Thingiverse Mobile app (available via Google Play and Apple App Store).<|endoftext|>A lot of media attention is placed on Jeff McCaughey — a 6'1, 265 pound, 231 pound, quarterback at Kentucky — despite the fact that he's only four years removed from becoming Kentucky's major-college transfer.\n",
      "\n",
      "What McCaughey hasn't done beyond tryouts is workouts. He's also scored just 2,359 yards and 6 touchdowns on 323 carries. With that can start to feel that over. He's a solid valedictorian athlete, just 3 universities behind him after Cal for Thomas Sowell. That he's strong as a support recruit might take a bit of pressure off his shoulders. If he stays healthy, he could even end up joining No. 3 St. Joseph's Prep in the Big Ten… or even Notre Dame…\n",
      "\n",
      "News of McCaughey's workouts will come after six convincing seasons out of La Salle High School in northern New Jersey, where he was physically and mentally subject to mass incarceration. Longhorns Entertainment released an exclusive video featuring a glimpse into McCaughey's transition as the teams moved to outdoor courts and fueled by a strong combination of his size and smarts.\n",
      "\n",
      "\"I got to play 14, 15, seasons west – all. Anywhere from every college even a mid-major probably every 3-5 years,\" McCaughey said. \"You always have to be patient, T.J.\"\n",
      "\n",
      "McDoughey, 24, grew up in Newark, N.J., where he became friends with neighbors and played baseball regularly until those experiences came to an end. After living in London, where he returned to downtown as a 10-year-old and started doing overdosing on drugs and alcohol in Skyway, he purchased a moped for special occasions with his brother and his alma mater. Coming into the NFL, McCaughey briefly represented Kentucky as a pro football player. He formally entered the league at the time in 2006, and pitched for the Thundering Herd' when he was picked by the first team in the first round of the 2007 college draft. To his credit, being drafted is a dream come true for most basketball prospects.\n",
      "\n",
      "This rise and fall of Japan Education Part.\n",
      "\n",
      "While going pro, McCaughey struggled with his double team and showed signs of self-pity. After scoring one of his first career 2,384 total rushing yards, McCaughey was drafted by the government in 2002 and eventually signed with UCLA in 2003. After an illustrious college career there brewed up on the campus of Southern New Jersey, McCaughey quickly moved back into Los Angeles in preparation for blue chip UCLA, but the team wouldn't allow him to play opening year. Those two programs left the 5's on board for Lansing and committed to his squad. After a dismal 2009 season in college, Bolton said: \"Born in Fairfax, big year on 3/2/17. I turned 30 in May. Then we bought my car. Now it's training camp.\"\n",
      "\n",
      "Looking back, though, McCaughey's meteoric rise to Hoosier prominence represents something that the Bruins have never done before. New head coach Brian Shaw's renowned \"ball disposal system,\" systems that is by no means new, controlled the game and lines the ball up for blockers only to strip the defenders away or pass it to others. Just this past fall it appears that McCaughey has brought that philosophy to Kentucky – and possibly LSU – as well.\n",
      "\n",
      "Instead, it didn't matter what pass protection he was held to. The Quarterback Edition took McCaughey's pass rush system and refined its play design to create a stiffer bully in the pocket than\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close>"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('output.txt','w')\n",
    "sample_model(nsamples=5)\n",
    "file.close"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMlISGtKhswvog8Yc8s8kde",
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
